% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_cmab_ts.R
\name{ContextualThompsonSamplingPolicy}
\alias{ContextualThompsonSamplingPolicy}
\title{Policy: Contextual Thompson Sampling with Linear Payoffs}
\description{
\code{ContextualThompsonSamplingPolicy} implements Thompson Sampling with Linear
Payoffs, following Agrawal and Goyal (2011).
Thompson Sampling with Linear Payoffs is a contextual Thompson Sampling multi-armed bandit
Policy which assumes the underlying relationship between rewards and contexts
are linear. Check the reference for more details.
}
\section{Usage}{

\preformatted{
policy <- ContextualThompsonSamplingPolicy$new(delta, R, epsilon)
}
}

\section{Arguments}{


\describe{
\item{\code{delta}}{
numeric; 0 < \code{delta} <= 1.
With probability 1 - delta, ContextualThompsonSamplingPolicy satisfies the theoretical regret bound.
}
\item{\code{R}}{
numeric; \code{R} >= 0.
Assume that the residual  \eqn{ri(t) - bi(t)^T \hat{\mu}} is R-sub-gaussian.
In this case, \eqn{R^2} represents the variance for residuals of the linear model \eqn{bi(t)^T}.
}
\item{\code{epsilon}}{
numeric; 0 < \code{epsilon} < 1
If the total trials T is known, we can choose epsilon = 1/ln(T).
}
}
}

\section{Methods}{


\describe{
\item{\code{new(...)}}{ instantiates a new \code{ContextualThompsonSamplingPolicy} instance.
Arguments defined in the Arguments section above.}
}

\describe{
\item{\code{set_parameters(context_params)}}{
initialization of policy parameters, utilising \code{context_params$k} (number of arms) and
\code{context_params$d} (number of context features).
}
}

\describe{
\item{\code{get_action(t,context)}}{
selects an arm based on \code{self$theta} and \code{context}, returning the index of the selected arm
in \code{action$choice}. The {context} argument consists of a list with \code{context$k} (number of arms),
\code{context$d} (number of features), and the feature matrix \code{context$X} with dimensions
\eqn{d \times k}{d x k}.
}
}

\describe{
\item{\code{set_reward(t, context, action, reward)}}{
updates parameter list \code{theta} in accordance with the current \code{reward$reward},
\code{action$choice} and the feature matrix \code{context$X} with dimensions
\eqn{d \times k}{d x k}. Returns the updated \code{theta}.
}
}
}

\examples{
\donttest{
horizon            <- 1000L
simulations        <- 10L

bandit             <- ContextualLogitBandit$new(k = 5, d = 5)

delta              <- 0.5
R                  <- 0.01
epsilon            <- 0.5

policy             <- ContextualThompsonSamplingPolicy$new(delta, R, epsilon)

agent              <- Agent$new(policy, bandit)

simulation         <- Simulator$new(agents, horizon, simulations)
history            <- simulation$run()

plot(history, type = "cumulative", regret = FALSE, rate = TRUE)
}
}
\references{
Shipra Agrawal, and Navin Goyal. "Thompson Sampling for Contextual Bandits with Linear Payoffs." Advances in Neural Information Processing Systems 24. 2011.

Implementation follows linthompsamp from \url{https://github.com/ntucllab/striatum}
}
\seealso{
Core contextual classes: \code{\link{Bandit}}, \code{\link{Policy}}, \code{\link{Simulator}},
\code{\link{Agent}}, \code{\link{History}}, \code{\link{Plot}}

Other contextual subclasses: \code{\link{ContextualEpochGreedyPolicy}},
  \code{\link{ContextualThompsonHybrid}},
  \code{\link{ContinuumBandit}},
  \code{\link{EpsilonFirstPolicy}},
  \code{\link{EpsilonGreedyPolicy}},
  \code{\link{Exp3Policy}},
  \code{\link{GittinsBrezziLaiPolicy}},
  \code{\link{LifPolicy}},
  \code{\link{LinUCBDisjointOptimizedPolicy}},
  \code{\link{LinUCBDisjointPolicy}},
  \code{\link{LinUCBGeneralPolicy}},
  \code{\link{LinUCBHybridOptimizedPolicy}},
  \code{\link{LinUCBHybridPolicy}},
  \code{\link{LinUCBOFULPolicy}},
  \code{\link{OfflinePolicyEvaluatorBandit}},
  \code{\link{OraclePolicy}}, \code{\link{RandomPolicy}},
  \code{\link{SoftmaxPolicy}},
  \code{\link{ThompsonSamplingPolicy}},
  \code{\link{UCB1Policy}}
}
