<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Getting started: running simulations • contextual</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Getting started: running simulations">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">contextual</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.9.8.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/cmabs.html">Demo: Basic Synthetic cMAB Policies</a>
    </li>
    <li>
      <a href="../articles/cmabsoffline.html">Demo: Offline cMAB LinUCB evaluation</a>
    </li>
    <li>
      <a href="../articles/eckles_kaptein.html">Demo: MAB Replication Eckles &amp; Kaptein (Bootstrap Thompson Sampling)</a>
    </li>
    <li>
      <a href="../articles/epsilongreedy.html">Demo: Basic Epsilon Greedy</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Getting started: running simulations</a>
    </li>
    <li>
      <a href="../articles/mabs.html">Demo: MAB Policies Comparison</a>
    </li>
    <li>
      <a href="../articles/ml10m.html">Demo: MovieLens 10M Dataset</a>
    </li>
    <li>
      <a href="../articles/offline_depaul_movies.html">Demo: Offline cMAB: CarsKit DePaul Movie Dataset</a>
    </li>
    <li>
      <a href="../articles/replication.html">Offline evaluation: Replication of Li et al 2010</a>
    </li>
    <li>
      <a href="../articles/simpsons.html">Demo: Bandits, Propensity Weighting &amp; Simpson's Paradox in R</a>
    </li>
    <li>
      <a href="../articles/sutton_barto.html">Demo: Replication Sutton &amp; Barto, Reinforcement Learning: An Introduction, Chapter 2</a>
    </li>
    <li>
      <a href="../articles/website_optimization.html">Demo: Replication of John Myles White, Bandit Algorithms for Website Optimization</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
<li>
  <a href="../articles/only_pkgdown/faq.html">FAQ</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Nth-iteration-labs/contextual">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Getting started: running simulations</h1>
                        <h4 class="author">Robin van Emden</h4>
            
            <h4 class="date">2020-03-04</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/vignettes/introduction.Rmd"><code>vignettes/introduction.Rmd</code></a></small>
      <div class="hidden name"><code>introduction.Rmd</code></div>

    </div>

    
    
<p>Though contextual has been developed to ease the implementation and comparison of custom Bandits and Policies, it can also be used to evaluate policies from contextual’s growing policy library. In the current vignette we will demonstrate how to run such simulations in an online advertising related scenario. Generally, the goal in online advertising is to determine which out of several ads to serve a visitor to a particular web page. Translated to a bandit setting, in online advertising:</p>
<ul>
<li>The context is usually determined by visitor and web page characteristics.</li>
<li>Arms are represented by the pool of available ads.</li>
<li>An action equals a shown add.</li>
<li>Rewards are determined by a visitor clicking (a reward of 1) or not clicking (a reward of 0) on the shown ad.</li>
</ul>
<p>For the current tutorial, we limit the number of advertisements we want to evaluate to three, and set ourselves the objective of finding which policy would offer the highest total click-through rate over four hundred impressions.</p>
<div id="evaluating-context-free-policies" class="section level4">
<h4 class="hasAnchor">
<a href="#evaluating-context-free-policies" class="anchor"></a>Evaluating context-free policies</h4>
<p>Before we are able to evaluate any policies, we first need to model the three ads—each with a different probability of generating a click—as the arms of a bandit. For the current simulation we choose to model the ads with the weight-based ContextualBernoulliBandit, as this allows us to set weights that determine the average reward probability of each arm. As can be observed in the source code below, for the current simulation, we set the weights of the arms to respectively <span class="math inline">\(\theta_1 = 0.8\)</span>, <span class="math inline">\(\theta_2 = 0.4\)</span> and <span class="math inline">\(\theta_3 = 0.2\)</span>.</p>
<p>We also choose two context-free policies to evaluate and compare:</p>
<ul>
<li><p>EpsilonFirstPolicy: explores the three ads uniformly at random for a preset period and from thereon exploits the ad with the best click-through rate (a type of policy also known as an A/B test). For the current scenario, we set the exploration period to one hundred impressions.</p></li>
<li><p>EpsilonGreedyPolicy: explores one of the ads uniformly at random <span class="math inline">\(\epsilon\)</span> of the time and exploits the ad with the best current click-through rate <span class="math inline">\(1 - \epsilon\)</span> of the time. Here, we will set <span class="math inline">\(\epsilon = 0.4\)</span>.</p></li>
</ul>
<p>Next, we assign the bandit and the two policy instances to two agents. Finally, we assign a list holding both agents to a Simulator instance, set the simulator’s horizon to four hundred and the number of repeats to ten thousand, run the simulation, and plot() its results:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load and attach the contextual package.</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(contextual)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># Define for how long the simulation will run.</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>horizon &lt;-<span class="st"> </span><span class="dv">400</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Define how many times to repeat the simulation.</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>simulations &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Define the probability that each ad will be clicked.</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>click_probabilities &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.8</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># Initialize a ContextualBernoulliBandit</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>bandit &lt;-<span class="st"> </span>ContextualBernoulliBandit<span class="op">$</span><span class="kw">new</span>(<span class="dt">weights =</span> click_probabilities)</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co"># Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>eg_policy &lt;-<span class="st"> </span>EpsilonGreedyPolicy<span class="op">$</span><span class="kw">new</span>(<span class="dt">epsilon =</span> <span class="fl">0.4</span>)</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co"># Initialize an EpsilonFirstPolicy with a 100 step exploration period.</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>ef_policy &lt;-<span class="st"> </span>EpsilonFirstPolicy<span class="op">$</span><span class="kw">new</span>(<span class="dt">time_steps =</span> <span class="dv">100</span>)</span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co"># Initialize two Agents, binding each policy to a bandit.</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>ef_agent &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(ef_policy, bandit)</span>
<span id="cb1-17"><a href="#cb1-17"></a>eg_agent &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(eg_policy, bandit)</span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="co"># Assign both agents to a list.</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>agents &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(ef_agent, eg_agent)</span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="co"># Initialize Simulator with agent list, horizon, and nr of simulations.</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>simulator &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agents, horizon, simulations, <span class="dt">do_parallel =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="co"># Now run the simulator.</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>history &lt;-<span class="st"> </span>simulator<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()</span>
<span id="cb1-24"><a href="#cb1-24"></a><span class="co"># Finally, plot the average reward per time step t</span></span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(history, <span class="dt">type =</span> <span class="st">"average"</span>, <span class="dt">regret =</span> <span class="ot">FALSE</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="co"># And the cumulative reward rate, which equals the Click Through Rate)</span></span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(history, <span class="dt">type =</span> <span class="st">"cumulative"</span>, <span class="dt">regret =</span> <span class="ot">FALSE</span>, <span class="dt">rate =</span> <span class="ot">TRUE</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="contextual-fig-1.png" style="width:98.0%"></p>
<p>As can be observed in the figure above, within our horizon of <span class="math inline">\(T = 400\)</span>, EpsilonFirstPolicy has accumulated more rewards than EpsilonGreedytPolicy. It is easy to see why: The winning arm is better than the other two—by a margin. That means that EpsilonFirstPolicy has no difficulty in finding the optimal arm within its exploration period of one hundred impressions. Up to that point, EpsilonGreedyPolicy still had the advantage of a headstart, as it was already able to exploit for <span class="math inline">\(1- \epsilon\)</span> or sixty percent of the time. But from one hundred impressions on, EpsilonFirstPolicy switches from full exploration to full exploitation mode. In contrast to EpsilonGreedyPolicy, it is now able to exploit the arm that proved best during exploration all of the time. As a result, it is able catch up with (and then surpass) the rewards accumulated by EpsilonGreedyPolicy within less than one hundred and fifty impressions.</p>
</div>
<div id="adding-context" class="section level4">
<h4 class="hasAnchor">
<a href="#adding-context" class="anchor"></a>Adding context</h4>
<p>If that is all we know of our visitors, we expect the results to be stationary over time, and these are the only policies available, the choice is clear: for this scenario, you would pick EpsilonFirstPolicy (Also: if the bandit model represents our visitors’ click behavior realistically, if our policies’ parameters are optimal, etcetera). However, if we have contextual information on our visitors—for instance, their age—we might be able to do better. Let us suggest that we expect that some of our ads are more effective for older visitors, and other ads more effective for younger visitors.</p>
<p>To incorporate this expectation in our simulation, we need to change the way our bandit generates its rewards. Fortunately, in the case of ContextualBernoulliBandit, the introduction of two contextual features only requires the addition of a single row to its weight matrix—as ContextualBernoulliBandit parses each of the <span class="math inline">\(d\)</span> rows of its weight matrix as a binary contextual feature, each randomly selected or sampled <span class="math inline">\(1/d\)</span> of the time.</p>
<p>We therefore also include the contextual LinUCBDisjointPolicy, which, in assuming its reward function is a linear function of the context, is able to incorporate our new contextual information into its decision-making process. Now let us rerun the simulation:</p>
<pre><code>#                                  +-----+----+-----------&gt; ads: k = 3
#                                  |     |    |
click_probs         &lt;- matrix(c(  0.2,  0.3, 0.1,     # --&gt; d1: old   (p=.5)
                                  0.6,  0.1, 0.1   ), # --&gt; d2: young (p=.5)
                                                      #     features: d = 2

                                  nrow = 2, ncol = 3, byrow = TRUE)

# Initialize a ContextualBernoulliBandit with contextual weights
context_bandit      &lt;- ContextualBernoulliBandit$new(weights = click_probs)
# Initialize LinUCBDisjointPolicy
lucb_policy         &lt;- LinUCBDisjointPolicy$new(0.6)
# Initialize three Agents, binding each policy to a bandit.
ef_agent            &lt;- Agent$new(ef_policy, context_bandit)
eg_agent            &lt;- Agent$new(eg_policy, context_bandit)
lucb_agent          &lt;- Agent$new(lucb_policy, context_bandit)
# Assign all agents to a list.
agents              &lt;- list(ef_agent, eg_agent, lucb_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator           &lt;- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
# Now run the simulator.
history             &lt;- simulator$run()
# And plot the cumulative reward rate again.
plot(history, type = "cumulative", regret = FALSE, rate = TRUE)</code></pre>
<p><img src="contextual-fig-2.png" style="width:98.0%"></p>
<p>As can be observed in the figure above, both context-free bandit’s do indeed not do better than before. On the other hand, LinUCBDisjointPolicy does very well, as it is able to map its rewards to the available contextual features.</p>
<p>Of course, the simulations in the current vignette are not very realistic. One way to ameliorate this would be to write a Bandit subclass with a more complex generative model. More on that in the vignette “Creating your own Policies and Bandits”. Another option would be to evaluate policies on offline datasets. Which is covered in the vignette “Evaluating policies with offline datasets”.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Robin van Emden.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: '7d3ef95d72d5a68e705ce87f9919b959',
    indexName: 'nth_iteration_labs_contextual',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
