<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Demo: Bandits, Propensity Weighting &amp; Simpson's Paradox in R • contextual</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.3/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Demo: Bandits, Propensity Weighting &amp; Simpson's Paradox in R">
<meta property="og:description" content="contextual">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">contextual</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.9.8.4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/cmabs.html">Demo: Basic Synthetic cMAB Policies</a>
    </li>
    <li>
      <a href="../articles/cmabsoffline.html">Demo: Offline cMAB LinUCB evaluation</a>
    </li>
    <li>
      <a href="../articles/eckles_kaptein.html">Demo: MAB Replication Eckles &amp; Kaptein (Bootstrap Thompson Sampling)</a>
    </li>
    <li>
      <a href="../articles/epsilongreedy.html">Demo: Basic Epsilon Greedy</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Getting started: running simulations</a>
    </li>
    <li>
      <a href="../articles/mabs.html">Demo: MAB Policies Comparison</a>
    </li>
    <li>
      <a href="../articles/ml10m.html">Demo: MovieLens 10M Dataset</a>
    </li>
    <li>
      <a href="../articles/offline_depaul_movies.html">Demo: Offline cMAB: CarsKit DePaul Movie Dataset</a>
    </li>
    <li>
      <a href="../articles/replication.html">Offline evaluation: Replication of Li et al 2010</a>
    </li>
    <li>
      <a href="../articles/simpsons.html">Demo: Bandits, Propensity Weighting &amp; Simpson's Paradox in R</a>
    </li>
    <li>
      <a href="../articles/sutton_barto.html">Demo: Replication Sutton &amp; Barto, Reinforcement Learning: An Introduction, Chapter 2</a>
    </li>
    <li>
      <a href="../articles/website_optimization.html">Demo: Replication of John Myles White, Bandit Algorithms for Website Optimization</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
<li>
  <a href="../articles/only_pkgdown/faq.html">FAQ</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Nth-iteration-labs/contextual">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="simpsons_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Demo: Bandits, Propensity Weighting &amp; Simpson’s Paradox in R</h1>
                        <h4 class="author">Robin van Emden</h4>
            
            <h4 class="date">2020-07-25</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/vignettes/simpsons.Rmd"><code>vignettes/simpsons.Rmd</code></a></small>
      <div class="hidden name"><code>simpsons.Rmd</code></div>

    </div>

    
    
<p>Imagine a website with Sport and Movie related articles where the “actual” preference of men and women for Sport and Movie articles is the following:</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="no">Contexts</span>   <span class="kw">|</span> <span class="fu">Sport</span> (<span class="no">arm</span>) <span class="kw">|</span>  <span class="fu">Movie</span> (<span class="no">arm</span>)
-----------------------------------------
<span class="no">Male</span>       <span class="kw">|</span> <span class="fl">0.4</span>         <span class="kw">|</span>  <span class="fl">0.3</span>
<span class="no">Female</span>     <span class="kw">|</span> <span class="fl">0.8</span>         <span class="kw">|</span>  <span class="fl">0.7</span></pre></body></html></div>
<p>In other words, both Male and Female visitors actually prefer Sports articles over Movie articles. When visitors are randomly assigned to types of articles, the overall CTR rate per category reflects this:</p>
<div class="sourceCode" id="cb2"><html><body><pre class="r">Contexts   | Sport (arm) |  Movie (arm)
-----------------------------------------
Male       | 0.4 x 0.5   |  0.3 x 0.5
Female     | 0.8 x 0.5   |  0.7 x 0.5
-----------------------------------------
CTR total  | 0.6         |  0.5</pre></body></html></div>
<p>Now suggest the site’s editor just “knows” that men like sports, and women like movie related articles. So the editor has some business logic implemented, assigning Movie related articles, on average, to 75% of Female visitors, and Sports articles, on average, to 75% of Male visitors:</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r">Contexts   | Sport (arm) |  Movie (arm)
-----------------------------------------
Male       | 0.4 x 0.75  |  0.3 x 0.25
Female     | 0.8 x 0.25  |  0.7 x 0.75
-----------------------------------------
CTR total  | 0.5         |  0.6</pre></body></html></div>
<p>This results in a higher CTR for movies than for Sports related articles - even though these CTR’s do not actually reflect the overall preferences of website visitors, but rather the editor’s prejudice.</p>
<p>A clear example of <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Simpson’s Paradox</a>!</p>
<p>Below an R code based illustration (making use of our “<a href="https://github.com/Nth-iteration-labs/contextual">contextual</a>” bandit package) of how Simpson’s Paradox could give rise to biased logged data, resulting in biased offline evaluations of bandit policies. Next, we demonstrate how <a href="https://en.wikipedia.org/wiki/Inverse_probability_weighting">inverse propensity weighting</a> can help make such data usable for offline evaluation after all.</p>
<div id="original-bandit-weights" class="section level2">
<h2 class="hasAnchor">
<a href="#original-bandit-weights" class="anchor"></a>Original bandit weights</h2>
<p>Set up simulation bandit weights representing Male and Female actual preferences for Sports and Movies:</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r">
<span class="no">horizon</span>                           <span class="kw">&lt;-</span> <span class="fl">10000L</span>
<span class="no">simulations</span>                       <span class="kw">&lt;-</span> <span class="fl">1L</span>

<span class="co">#                    S----M------------&gt; Arm 1:   Sport</span>
<span class="co">#                    |    |              Arm 2:   Movie</span>
<span class="co">#                    |    |</span>
<span class="no">weights</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>( <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.4</span>, <span class="fl">0.3</span>,    <span class="co">#-----&gt; Context: Male</span>
                     <span class="fl">0.8</span>, <span class="fl">0.7</span>),   <span class="co">#-----&gt; Context: Female</span>

                     <span class="kw">nrow</span> <span class="kw">=</span> <span class="fl">2</span>, <span class="kw">ncol</span> <span class="kw">=</span> <span class="fl">2</span>, <span class="kw">byrow</span> <span class="kw">=</span> <span class="fl">TRUE</span>)</pre></body></html></div>
<p>These weights will be fed to contextual’s <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/R/bandit_cmab_bernoulli.R">ContextualBernoulliBandit</a> so it can simulate clicks by Male and Female according to their preferences per category.</p>
</div>
<div id="generate-data-by-running-a-fully-random-online-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#generate-data-by-running-a-fully-random-online-policy" class="anchor"></a>Generate data by running a fully random online policy</h2>
<p>Let’s first run contextual’s basic <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/R/policy_random.R">random policy</a> against the bandit that models actual visitor’s preferences. This random policy assigns Males and Females fully at random to either Sport or Movie articles:</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r">policy                            &lt;- RandomPolicy$new()
bandit                            &lt;- ContextualBernoulliBandit$new(weights = weights)
agent                             &lt;- Agent$new(policy, bandit, "Random")

simulation                        &lt;- Simulator$new(agent, horizon, simulations, 
                                                   save_context = TRUE, do_parallel = F)
                                                   
history                           &lt;- simulation$run()

Simulation horizon: 10000
Number of simulations: 1
Number of batches: 1
Starting main loop.
Completed simulation in 0:00:01.781

u_dt                              &lt;- history$get_data_table()

print("1a. Unbiased data generation.")
[1] "1a. Unbiased data generation."

print(paste("Sport:",sum(u_dt[choice==1]$reward)/nrow(u_dt[choice==1]))) 
[1] "Sport: 0.603323988786544"   # 0.6 CTR Sport - equals preferences!

print(paste("Movie:",sum(u_dt[choice==2]$reward)/nrow(u_dt[choice==2]))) 
[1] "Movie: 0.501997602876548"   # 0.5 CTR Movie - equals preferences!</pre></body></html></div>
<p>The results are clear: when running the random policy, the logged data accurately represents visitor’s preferences.</p>
</div>
<div id="using-the-random-policys-offline-logged-data-to-evaluate-another-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#using-the-random-policys-offline-logged-data-to-evaluate-another-policy" class="anchor"></a>Using the random policy’s offline logged data to evaluate another policy</h2>
<p>The previous simulation produced a data.table with fully randomised historical data. Let’s use this data to evaluate some other policy:</p>
<div class="sourceCode" id="cb6"><html><body><pre class="r">
f                                 &lt;- formula("reward ~ choice | X.1 + X.2")
bandit                            &lt;- OfflineReplayEvaluatorBandit$new(formula = f,
                                                                      data = u_dt, 
                                                                      k = 2 , d = 2)

policy                            &lt;- EpsilonGreedyPolicy$new(0.1)
agent                             &lt;- Agent$new(policy, bandit, "OfflineLinUCB")
 
simulation                        &lt;- Simulator$new(agent, horizon, simulations, do_parallel = F)
history                           &lt;- simulation$run()

Simulation horizon: 10000
Number of simulations: 1
Number of batches: 1
Starting main loop.
Completed simulation in 0:00:01.606

ru_dt                             &lt;- history$get_data_table()
 
print("1b. Offline unbiased policy evaluation.")
[1] "1b. Offline unbiased policy evaluation."
 
print(paste("Sport:",sum(ru_dt[choice==1]$reward)/nrow(ru_dt[choice==1])))
[1] "Sport: 0.602566799915843"   # 0.6 CTR Sport - equals preferences!

print(paste("Movie:",sum(ru_dt[choice==2]$reward)/nrow(ru_dt[choice==2]))) 
[1] "Movie: 0.493589743589744"   # 0.5 CTR Movie - equals preferences!</pre></body></html></div>
<p>Accurate numbers again: clearly, the logged data from a randomizing policy can be used to test other ‘off-policy’ algorithms.</p>
</div>
<div id="generate-data-by-running-a-biased-online-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#generate-data-by-running-a-biased-online-policy" class="anchor"></a>Generate data by running a biased online policy</h2>
<p>Now suggest some editor just "knows’ that men like Sport, and women like Movie. So some business logic was added to the site assigning Movie related articles, on average, to 75% of Female visitors, and Sport articles, on average, to 75% of Male visitors.</p>
<p>This business logic might be implemented through the following policy:</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="no">BiasedPolicy</span>                      <span class="kw">&lt;-</span> <span class="kw pkg">R6</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/R6/man/R6Class.html">R6Class</a></span>(
  <span class="kw">portable</span> <span class="kw">=</span> <span class="fl">FALSE</span>,
  <span class="kw">class</span> <span class="kw">=</span> <span class="fl">FALSE</span>,
  <span class="kw">inherit</span> <span class="kw">=</span> <span class="no">RandomPolicy</span>,
  <span class="kw">public</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(
   <span class="kw">class_name</span> <span class="kw">=</span> <span class="st">"BiasedPolicy"</span>,
   <span class="kw">get_action</span> <span class="kw">=</span> <span class="kw">function</span>(<span class="no">t</span>, <span class="no">context</span>) {
     <span class="kw">if</span>(<span class="no">context</span>$<span class="no">X</span>[<span class="fl">1</span>]<span class="kw">==</span><span class="fl">1</span>) {           <span class="co"># 1: Male || 0: Female.</span>
       <span class="no">prob</span>                      <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.75</span>,<span class="fl">0.25</span>) <span class="co"># Editor thinks men like Sport articles more.</span>
     } <span class="kw">else</span> {
       <span class="no">prob</span>                      <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.25</span>,<span class="fl">0.75</span>) <span class="co"># Editor thinks women like Movie articles more.</span>
     }
     <span class="no">action</span>$<span class="no">choice</span>               <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample.int</a></span>(<span class="no">context</span>$<span class="no">k</span>, <span class="fl">1</span>, <span class="kw">replace</span> <span class="kw">=</span> <span class="fl">TRUE</span>, <span class="kw">prob</span> <span class="kw">=</span> <span class="no">prob</span>)
      <span class="co"># Store the propensity score for the current action too:</span>
      <span class="no">action</span>$<span class="no">propensity</span>           <span class="kw">&lt;-</span> <span class="no">prob</span>[<span class="no">action</span>$<span class="no">choice</span>]
      <span class="no">action</span>
    }
  )
)</pre></body></html></div>
<p>Now run this policy against the Bandit modeling actual visitor preferences:</p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"> 
policy                            &lt;- BiasedPolicy$new()
bandit                            &lt;- ContextualBernoulliBandit$new(weights = weights)
agent                             &lt;- Agent$new(policy, bandit, "Random")

simulation                        &lt;- Simulator$new(agent, horizon, simulations, 
                                                   save_context = TRUE, do_parallel = F)
history                           &lt;- simulation$run()

Simulation horizon: 10000
Number of simulations: 1
Number of batches: 1
Starting main loop.
Completed simulation in 0:00:01.954
 
b_dt                              &lt;- history$get_data_table()

print("2a. Biased data generation.")
[1] "2a. Biased data generation."
 
print(paste("Sport:",sum(b_dt[choice==1]$reward)/nrow(b_dt[choice==1]))) 
[1] "Sport: 0.506446414182111"  # 0.5 CTR Sport - Simpson's paradox at work

print(paste("Movie:",sum(b_dt[choice==2]$reward)/nrow(b_dt[choice==2]))) 
[1] "Movie: 0.600675138999206"  # 0.6 CTR Movie - Simpson's..</pre></body></html></div>
<p>Clearly, the BiasedPolicy gives rise to, well, biased results! If you’d only be able to look at the data, without knowing of the biased business logic, you’d falsely conclude Movies is more popular then Sports, overall.</p>
</div>
<div id="using-the-biased-policys-offline-logged-data-to-evaluate-another-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#using-the-biased-policys-offline-logged-data-to-evaluate-another-policy" class="anchor"></a>Using the biased policy’s offline logged data to evaluate another policy</h2>
<p>This time, the simulation generated a data.table with biased data. Let’s see what happens if we use this data to evaluate some other policy:</p>
<div class="sourceCode" id="cb9"><html><body><pre class="r">f                                 &lt;- formula("reward ~ choice | X.1 + X.2")
bandit                            &lt;- OfflineReplayEvaluatorBandit$new(formula = f, 
                                                                      data = b_dt, 
                                                                      k = 2 , d = 2)
policy                            &lt;- EpsilonGreedyPolicy$new(0.1)
agent                             &lt;- Agent$new(policy, bandit, "rb")
 
simulation                        &lt;- Simulator$new(agent, horizon, simulations, do_parallel = F)
history                           &lt;- simulation$run()

Simulation horizon: 10000
Number of simulations: 1
Number of batches: 1
Starting main loop.
Completed simulation in 0:00:01.478

rb_dt                             &lt;- history$get_data_table()
 
print("2b. Offline biased policy evaluation.")
[1] "2b. Offline biased policy evaluation."
 
print(paste("Sport:",sum(rb_dt[choice==1]$reward)/nrow(rb_dt[choice==1]))) 
[1] "Sport: 0.5"  # 0.5 CTR Sport - Simpson's paradox, again!

print(paste("Movie:",sum(rb_dt[choice==2]$reward)/nrow(rb_dt[choice==2]))) 
[1] "Movie: 0.602175277138674" # 0.6 CTR Sport - Simpson's paradox, again!</pre></body></html></div>
<p>The bias has propagated itself! So, does that mean it is not possible to use the “biased” data to evaluate other, off-line policies? That would severely limit the number of data sets for use in offline evaluation.</p>
</div>
<div id="repairing-the-biased-policys-logged-data-with-inverse-probability-weights" class="section level2">
<h2 class="hasAnchor">
<a href="#repairing-the-biased-policys-logged-data-with-inverse-probability-weights" class="anchor"></a>Repairing the biased policy’s logged data with inverse probability weights</h2>
<p>Luckily, inverse propensity score weighting enables us to use propensity scores to obtain unbiased estimates of the original preferences of Male and Female visitors. That is, since our biased policy actually saved the propensity (“the probability of a unit being assigned to a particular treatment or category”) with which a certain category was chosen, we can correct for this bias while “replaying” the data. In “contextual”, there are several types of offline bandits that are able to use either such presaved propensities, or estimate propensities based on certain properties of the dataset. Here, we use its basic “<a href="https://github.com/Nth-iteration-labs/contextual/blob/master/R/bandit_offline_propensity_weighting.R">OfflinePropensityWeightingBandit</a>”:</p>
<div class="sourceCode" id="cb10"><html><body><pre class="r">f                                 &lt;- formula("reward ~ choice | X.1 + X.2 | propensity")
bandit                            &lt;- OfflinePropensityWeightingBandit$new(formula = f, data = b_dt,
                                                                          k = 2 , d = 2)
policy                            &lt;- EpsilonGreedyPolicy$new(0.1)
agent                             &lt;- Agent$new(policy, bandit, "prop")
 
simulation                        &lt;- Simulator$new(agent, horizon, simulations, do_parallel = F)
history                           &lt;- simulation$run()

Simulation horizon: 10000
Number of simulations: 1
Number of batches: 1
Starting main loop.
Completed simulation in 0:00:01.257

prop_dt                           &lt;- history$get_data_table()
 
print("2c. Offline biased policy evaluation, inverse propensity scores.")
[1] "2c. Offline biased policy evaluation, inverse propensity scores."
 
print(paste("Sport:",sum(prop_dt[choice==1]$reward)/nrow(prop_dt[choice==1])))
[1] "Sport: 0.618266176609179"  # 0.6 CTR Sport, representing actual preferences - yay!

print(paste("Movie:",sum(prop_dt[choice==2]$reward)/nrow(prop_dt[choice==2]))) 
[1] "Movie: 0.496500591177808" # 0.5 CTR Movie, again, representing actual preferences..</pre></body></html></div>
<p>Hurray - inverse propensity score weighting has removed the bias! In other words: if and where possible, save propensity scores to your log files when experimenting with online policies. You will thank yourself at a later time!</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Robin van Emden.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: '7d3ef95d72d5a68e705ce87f9919b959',
    indexName: 'nth_iteration_labs_contextual',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
