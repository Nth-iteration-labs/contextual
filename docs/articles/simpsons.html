<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Demo: Bandits, Propensity Weighting &amp; Simpson’s Paradox in R • contextual</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Demo: Bandits, Propensity Weighting &amp; Simpson’s Paradox in R">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">contextual</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.9.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/cmabs.html">Demo: Basic Synthetic cMAB Policies</a>
    </li>
    <li>
      <a href="../articles/cmabsoffline.html">Demo: Offline cMAB LinUCB evaluation</a>
    </li>
    <li>
      <a href="../articles/eckles_kaptein.html">Demo: MAB Replication Eckles &amp; Kaptein (Bootstrap Thompson Sampling)</a>
    </li>
    <li>
      <a href="../articles/epsilongreedy.html">Demo: Basic Epsilon Greedy</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Getting started: running simulations</a>
    </li>
    <li>
      <a href="../articles/mabs.html">Demo: MAB Policies Comparison</a>
    </li>
    <li>
      <a href="../articles/ml10m.html">Demo: MovieLens 10M Dataset</a>
    </li>
    <li>
      <a href="../articles/replication.html">Offline evaluation: Replication of Li et al 2010</a>
    </li>
    <li>
      <a href="../articles/simpsons.html">Demo: Bandits, Propensity Weighting &amp; Simpson's Paradox in R</a>
    </li>
    <li>
      <a href="../articles/sutton_barto.html">Demo: Replication Sutton &amp; Barto, Reinforcement Learning: An Introduction, Chapter 2</a>
    </li>
    <li>
      <a href="../articles/website_optimization.html">Demo: Replication of John Myles White, Bandit Algorithms for Website Optimization</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Nth-iteration-labs/contextual">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Demo: Bandits, Propensity Weighting &amp; Simpson’s Paradox in R</h1>
                        <h4 class="author">Robin van Emden</h4>
            
            <h4 class="date">2019-01-20</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/vignettes/simpsons.Rmd"><code>vignettes/simpsons.Rmd</code></a></small>
      <div class="hidden name"><code>simpsons.Rmd</code></div>

    </div>

    
    
<p>Imagine a website with Sport and Movie related articles where the “actual” preference of men and women for Sport and Movie articles is the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Contexts   <span class="op">|</span><span class="st"> </span><span class="kw">Sport</span> (arm) <span class="op">|</span><span class="st">  </span><span class="kw">Movie</span> (arm)
<span class="op">-----------------------------------------</span>
Male       <span class="op">|</span><span class="st"> </span><span class="fl">0.4</span>         <span class="op">|</span><span class="st">  </span><span class="fl">0.3</span>
Female     <span class="op">|</span><span class="st"> </span><span class="fl">0.8</span>         <span class="op">|</span><span class="st">  </span><span class="fl">0.7</span></code></pre></div>
<p>In other words, both Male and Female visitors actually prefer Sports articles over Movie articles. When visitors are randomly assigned to types of articles, the overall CTR rate per category reflects this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Contexts   <span class="op">|</span><span class="st"> </span><span class="kw">Sport</span> (arm) <span class="op">|</span><span class="st">  </span><span class="kw">Movie</span> (arm)
<span class="op">-----------------------------------------</span>
Male       <span class="op">|</span><span class="st"> </span><span class="fl">0.4</span> x <span class="fl">0.5</span>   <span class="op">|</span><span class="st">  </span><span class="fl">0.3</span> x <span class="fl">0.5</span>
Female     <span class="op">|</span><span class="st"> </span><span class="fl">0.8</span> x <span class="fl">0.5</span>   <span class="op">|</span><span class="st">  </span><span class="fl">0.7</span> x <span class="fl">0.5</span>
<span class="op">-----------------------------------------</span>
CTR total  <span class="op">|</span><span class="st"> </span><span class="fl">0.6</span>         <span class="op">|</span><span class="st">  </span><span class="fl">0.5</span></code></pre></div>
<p>Now suggest the site’s editor just “knows” that men like sports, and women like movie related articles. So the editor has some business logic implemented, assigning Movie related articles, on average, to 75% of Female visitors, and Sports articles, on average, to 75% of Male visitors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Contexts   <span class="op">|</span><span class="st"> </span><span class="kw">Sport</span> (arm) <span class="op">|</span><span class="st">  </span><span class="kw">Movie</span> (arm)
<span class="op">-----------------------------------------</span>
Male       <span class="op">|</span><span class="st"> </span><span class="fl">0.4</span> x <span class="fl">0.75</span>  <span class="op">|</span><span class="st">  </span><span class="fl">0.3</span> x <span class="fl">0.25</span>
Female     <span class="op">|</span><span class="st"> </span><span class="fl">0.8</span> x <span class="fl">0.25</span>  <span class="op">|</span><span class="st">  </span><span class="fl">0.7</span> x <span class="fl">0.75</span>
<span class="op">-----------------------------------------</span>
CTR total  <span class="op">|</span><span class="st"> </span><span class="fl">0.5</span>         <span class="op">|</span><span class="st">  </span><span class="fl">0.6</span></code></pre></div>
<p>This results in a higher CTR for movies than for Sports related articles - even though these CTR’s do not actually reflect the overall preferences of website visitors, but rather the editor’s prejudice.</p>
<p>A clear example of <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Simpson’s Paradox</a>!</p>
<p>Below an R code based illustration (making use of our “<a href="https://github.com/Nth-iteration-labs/contextual">contextual</a>” bandit package) of how Simpson’s Paradox could give rise to biased logged data, resulting in biased offline evaluations of bandit policies. Next, we demonstrate how <a href="https://en.wikipedia.org/wiki/Inverse_probability_weighting">inverse propensity weighting</a> can help make such data usable for offline evaluation after all.</p>
<div id="original-bandit-weights" class="section level2">
<h2 class="hasAnchor">
<a href="#original-bandit-weights" class="anchor"></a>Original bandit weights</h2>
<p>Set up simulation bandit weights representing Male and Female actual preferences for Sports and Movies:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> 
horizon                           &lt;-<span class="st"> </span>10000L
simulations                       &lt;-<span class="st"> </span>1L

<span class="co">#                    S----M------------&gt; Arm 1:   Sport</span>
<span class="co">#                    |    |              Arm 2:   Movie</span>
<span class="co">#                    |    |</span>
weights &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/matrix">matrix</a></span>( <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="fl">0.4</span>, <span class="fl">0.3</span>,    <span class="co">#-----&gt; Context: Male</span>
                     <span class="fl">0.8</span>, <span class="fl">0.7</span>),   <span class="co">#-----&gt; Context: Female</span>
 
                     <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>These weights will be fed to contextual’s <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/R/bandit_cmab_bernoulli.R">ContextualBernoulliBandit</a> so it can simulate clicks by Male and Female according to their preferences per category.</p>
</div>
<div id="generate-data-by-running-a-fully-random-online-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#generate-data-by-running-a-fully-random-online-policy" class="anchor"></a>Generate data by running a fully random online policy</h2>
<p>Let’s first run contextual’s basic <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/R/policy_random.R">random policy</a> against the bandit that models actual visitor’s preferences. This random policy assigns Males and Females fully at random to either Sport or Movie articles:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">policy                            &lt;-<span class="st"> </span>RandomPolicy<span class="op">$</span><span class="kw">new</span>()
bandit                            &lt;-<span class="st"> </span>ContextualBernoulliBandit<span class="op">$</span><span class="kw">new</span>(<span class="dt">weights =</span> weights)
agent                             &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(policy, bandit, <span class="st">"Random"</span>)

simulation                        &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agent, horizon, simulations, 
                                                   <span class="dt">save_context =</span> <span class="ot">TRUE</span>, <span class="dt">do_parallel =</span> F)
                                                   
history                           &lt;-<span class="st"> </span>simulation<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()

Simulation horizon<span class="op">:</span><span class="st"> </span><span class="dv">10000</span>
Number of simulations<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Number of batches<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Starting main loop.
Completed simulation <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">00</span><span class="op">:</span><span class="fl">01.781</span>

u_dt                              &lt;-<span class="st"> </span>history<span class="op">$</span><span class="kw"><a href="../reference/History.html">get_data_table</a></span>()

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="st">"1a. Unbiased data generation."</span>)
[<span class="dv">1</span>] <span class="st">"1a. Unbiased data generation."</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Sport:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(u_dt[choice<span class="op">==</span><span class="dv">1</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(u_dt[choice<span class="op">==</span><span class="dv">1</span>]))) 
[<span class="dv">1</span>] <span class="st">"Sport: 0.603323988786544"</span>   <span class="co"># 0.6 CTR Sport - equals preferences!</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Movie:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(u_dt[choice<span class="op">==</span><span class="dv">2</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(u_dt[choice<span class="op">==</span><span class="dv">2</span>]))) 
[<span class="dv">1</span>] <span class="st">"Movie: 0.501997602876548"</span>   <span class="co"># 0.5 CTR Movie - equals preferences!</span></code></pre></div>
<p>The results are clear: when running the random policy, the logged data accurately represents visitor’s preferences.</p>
</div>
<div id="using-the-random-policys-offline-logged-data-to-evaluate-another-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#using-the-random-policys-offline-logged-data-to-evaluate-another-policy" class="anchor"></a>Using the random policy’s offline logged data to evaluate another policy</h2>
<p>The previous simulation produced a data.table with fully randomised historical data. Let’s use this data to evaluate some other policy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">
bandit                            &lt;-<span class="st"> </span>OfflineReplayEvaluatorBandit<span class="op">$</span><span class="kw">new</span>(u_dt,<span class="dv">2</span>,<span class="dv">2</span>)
policy                            &lt;-<span class="st"> </span>EpsilonGreedyPolicy<span class="op">$</span><span class="kw">new</span>(<span class="fl">0.1</span>)
agent                             &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(policy, bandit, <span class="st">"OfflineLinUCB"</span>)
 
simulation                        &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agent, horizon, simulations, <span class="dt">do_parallel =</span> F)
history                           &lt;-<span class="st"> </span>simulation<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()

Simulation horizon<span class="op">:</span><span class="st"> </span><span class="dv">10000</span>
Number of simulations<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Number of batches<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Starting main loop.
Completed simulation <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">00</span><span class="op">:</span><span class="fl">01.606</span>

ru_dt                             &lt;-<span class="st"> </span>history<span class="op">$</span><span class="kw"><a href="../reference/History.html">get_data_table</a></span>()
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="st">"1b. Offline unbiased policy evaluation."</span>)
[<span class="dv">1</span>] <span class="st">"1b. Offline unbiased policy evaluation."</span>
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Sport:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(ru_dt[choice<span class="op">==</span><span class="dv">1</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(ru_dt[choice<span class="op">==</span><span class="dv">1</span>])))
[<span class="dv">1</span>] <span class="st">"Sport: 0.602566799915843"</span>   <span class="co"># 0.6 CTR Sport - equals preferences!</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Movie:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(ru_dt[choice<span class="op">==</span><span class="dv">2</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(ru_dt[choice<span class="op">==</span><span class="dv">2</span>]))) 
[<span class="dv">1</span>] <span class="st">"Movie: 0.493589743589744"</span>   <span class="co"># 0.5 CTR Movie - equals preferences!</span></code></pre></div>
<p>Accurate numbers again: clearly, the logged data from a randomizing policy can be used to test other ‘off-policy’ algorithms.</p>
</div>
<div id="generate-data-by-running-a-biased-online-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#generate-data-by-running-a-biased-online-policy" class="anchor"></a>Generate data by running a biased online policy</h2>
<p>Now suggest some editor just “knows’ that men like Sport, and women like Movie. So some business logic was added to the site assigning Movie related articles, on average, to 75% of Female visitors, and Sport articles, on average, to 75% of Male visitors.</p>
<p>This business logic might be implemented through the following policy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">BiasedPolicy                      &lt;-<span class="st"> </span>R6<span class="op">::</span><span class="kw"><a href="https://www.rdocumentation.org/packages/R6/topics/R6Class">R6Class</a></span>(
  <span class="dt">portable =</span> <span class="ot">FALSE</span>,
  <span class="dt">class =</span> <span class="ot">FALSE</span>,
  <span class="dt">inherit =</span> RandomPolicy,
  <span class="dt">public =</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/list">list</a></span>(
   <span class="dt">class_name =</span> <span class="st">"BiasedPolicy"</span>,
   <span class="dt">get_action =</span> <span class="cf">function</span>(t, context) {
     <span class="cf">if</span>(context<span class="op">$</span>X[<span class="dv">1</span>]<span class="op">==</span><span class="dv">1</span>) {           <span class="co"># 1: Male || 0: Female.</span>
       prob                      &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="fl">0.75</span>,<span class="fl">0.25</span>) <span class="co"># Editor thinks men like Sport articles more.</span>
     } <span class="cf">else</span> {
       prob                      &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="fl">0.25</span>,<span class="fl">0.75</span>) <span class="co"># Editor thinks women like Movie articles more.</span>
     }
     action<span class="op">$</span>choice               &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sample">sample.int</a></span>(context<span class="op">$</span>k, <span class="dv">1</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> prob)
      <span class="co"># Store the propensity score for the current action too:</span>
      action<span class="op">$</span>propensity           &lt;-<span class="st"> </span>prob[action<span class="op">$</span>choice]
      action
    }
  )
) </code></pre></div>
<p>Now run this policy against the Bandit modeling actual visitor preferences:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> 
policy                            &lt;-<span class="st"> </span>BiasedPolicy<span class="op">$</span><span class="kw">new</span>()
bandit                            &lt;-<span class="st"> </span>ContextualBernoulliBandit<span class="op">$</span><span class="kw">new</span>(<span class="dt">weights =</span> weights)
agent                             &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(policy, bandit, <span class="st">"Random"</span>)

simulation                        &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agent, horizon, simulations, 
                                                   <span class="dt">save_context =</span> <span class="ot">TRUE</span>, <span class="dt">do_parallel =</span> F)
history                           &lt;-<span class="st"> </span>simulation<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()

Simulation horizon<span class="op">:</span><span class="st"> </span><span class="dv">10000</span>
Number of simulations<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Number of batches<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Starting main loop.
Completed simulation <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">00</span><span class="op">:</span><span class="fl">01.954</span>
 
b_dt                              &lt;-<span class="st"> </span>history<span class="op">$</span><span class="kw"><a href="../reference/History.html">get_data_table</a></span>()

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="st">"2a. Biased data generation."</span>)
[<span class="dv">1</span>] <span class="st">"2a. Biased data generation."</span>
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Sport:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(b_dt[choice<span class="op">==</span><span class="dv">1</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(b_dt[choice<span class="op">==</span><span class="dv">1</span>]))) 
[<span class="dv">1</span>] <span class="st">"Sport: 0.506446414182111"</span>  <span class="co"># 0.5 CTR Sport - Simpson's paradox at work</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Movie:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(b_dt[choice<span class="op">==</span><span class="dv">2</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(b_dt[choice<span class="op">==</span><span class="dv">2</span>]))) 
[<span class="dv">1</span>] <span class="st">"Movie: 0.600675138999206"</span>  <span class="co"># 0.6 CTR Movie - Simpson's..</span></code></pre></div>
<p>Clearly, the BiasedPolicy gives rise to, well, biased results! If you’d only be able to look at the data, without knowing of the biased business logic, you’d falsely conclude Sports is more popular then Movies, overall.</p>
</div>
<div id="using-the-biased-policys-offline-logged-data-to-evaluate-another-policy" class="section level2">
<h2 class="hasAnchor">
<a href="#using-the-biased-policys-offline-logged-data-to-evaluate-another-policy" class="anchor"></a>Using the biased policy’s offline logged data to evaluate another policy</h2>
<p>This time, the simulation generated a data.table with biased data. Let’s see what happens if we use this data to evaluate some other policy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bandit                            &lt;-<span class="st"> </span>OfflineReplayEvaluatorBandit<span class="op">$</span><span class="kw">new</span>(b_dt,<span class="dv">2</span>,<span class="dv">2</span>)
policy                            &lt;-<span class="st"> </span>EpsilonGreedyPolicy<span class="op">$</span><span class="kw">new</span>(<span class="fl">0.1</span>)
agent                             &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(policy, bandit, <span class="st">"rb"</span>)
 
simulation                        &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agent, horizon, simulations, <span class="dt">do_parallel =</span> F)
history                           &lt;-<span class="st"> </span>simulation<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()

Simulation horizon<span class="op">:</span><span class="st"> </span><span class="dv">10000</span>
Number of simulations<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Number of batches<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Starting main loop.
Completed simulation <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">00</span><span class="op">:</span><span class="fl">01.478</span>

rb_dt                             &lt;-<span class="st"> </span>history<span class="op">$</span><span class="kw"><a href="../reference/History.html">get_data_table</a></span>()
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="st">"2b. Offline biased policy evaluation."</span>)
[<span class="dv">1</span>] <span class="st">"2b. Offline biased policy evaluation."</span>
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Sport:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(rb_dt[choice<span class="op">==</span><span class="dv">1</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(rb_dt[choice<span class="op">==</span><span class="dv">1</span>]))) 
[<span class="dv">1</span>] <span class="st">"Sport: 0.5"</span>  <span class="co"># 0.5 CTR Sport - Simpson's paradox, again!</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Movie:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(rb_dt[choice<span class="op">==</span><span class="dv">2</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(rb_dt[choice<span class="op">==</span><span class="dv">2</span>]))) 
[<span class="dv">1</span>] <span class="st">"Movie: 0.602175277138674"</span> <span class="co"># 0.6 CTR Sport - Simpson's paradox, again!</span></code></pre></div>
<p>The bias has propagated itself! So, does that mean it is not possible to use the “biased” data to evaluate other, off-line policies? That would severely limit the number of data sets for use in offline evaluation.</p>
</div>
<div id="repairing-the-biased-policys-logged-data-with-inverse-probability-weights" class="section level2">
<h2 class="hasAnchor">
<a href="#repairing-the-biased-policys-logged-data-with-inverse-probability-weights" class="anchor"></a>Repairing the biased policy’s logged data with inverse probability weights</h2>
<p>Luckily, <a href="https://stats.stackexchange.com/questions/273367/intuitive-explanation-for-inverse-probability-of-treatment-weights-iptws-in-pr">inverse probability of treatment weighting</a> (IPTW) enables us to use propensity scores to obtain unbiased estimates of the original preferences of Male and Female visitors. That is, since our biased policy actually saved the propensity (“the probability of a unit being assigned to a particular treatment or category”) with which a certain category was chosen, we can correct for this bias while “replaying” the data. In “contextual”, there are several types of offline bandits that are able to use either such presaved propensities, or estimate propensities based on certain properties of the dataset. Here, we use its basic “<a href="https://github.com/Nth-iteration-labs/contextual/blob/master/R/bandit_offline_propensity_weighting.R">OfflinePropensityWeightingBandit</a>”:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bandit                            &lt;-<span class="st"> </span>OfflinePropensityWeightingBandit<span class="op">$</span><span class="kw">new</span>(b_dt,<span class="dv">2</span>,<span class="dv">2</span>, <span class="dt">stabilize =</span> T)
policy                            &lt;-<span class="st"> </span>EpsilonGreedyPolicy<span class="op">$</span><span class="kw">new</span>(<span class="fl">0.1</span>)
agent                             &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(policy, bandit, <span class="st">"prop"</span>)
 
simulation                        &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agent, horizon, simulations, <span class="dt">do_parallel =</span> F)
history                           &lt;-<span class="st"> </span>simulation<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()

Simulation horizon<span class="op">:</span><span class="st"> </span><span class="dv">10000</span>
Number of simulations<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Number of batches<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Starting main loop.
Completed simulation <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">00</span><span class="op">:</span><span class="fl">01.257</span>

prop_dt                           &lt;-<span class="st"> </span>history<span class="op">$</span><span class="kw"><a href="../reference/History.html">get_data_table</a></span>()
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="st">"2c. Offline biased policy evaluation, inverse propensity scores."</span>)
[<span class="dv">1</span>] <span class="st">"2c. Offline biased policy evaluation, inverse propensity scores."</span>
 
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Sport:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(prop_dt[choice<span class="op">==</span><span class="dv">1</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(prop_dt[choice<span class="op">==</span><span class="dv">1</span>])))
[<span class="dv">1</span>] <span class="st">"Sport: 0.618266176609179"</span>  <span class="co"># 0.6 CTR Sport, representing actual preferences - yay!</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste</a></span>(<span class="st">"Movie:"</span>,<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(prop_dt[choice<span class="op">==</span><span class="dv">2</span>]<span class="op">$</span>reward)<span class="op">/</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(prop_dt[choice<span class="op">==</span><span class="dv">2</span>]))) 
[<span class="dv">1</span>] <span class="st">"Movie: 0.496500591177808"</span> <span class="co"># 0.5 CTR Movie, again, representing actual preferences..</span></code></pre></div>
<p>Hurray - stabilized inverse propensity scoring has removed the bias! In other words: if and where possible, save propensity scores to your log files when experimenting with online policies. You will thank yourself at a later time!</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#original-bandit-weights">Original bandit weights</a></li>
      <li><a href="#generate-data-by-running-a-fully-random-online-policy">Generate data by running a fully random online policy</a></li>
      <li><a href="#using-the-random-policys-offline-logged-data-to-evaluate-another-policy">Using the random policy’s offline logged data to evaluate another policy</a></li>
      <li><a href="#generate-data-by-running-a-biased-online-policy">Generate data by running a biased online policy</a></li>
      <li><a href="#using-the-biased-policys-offline-logged-data-to-evaluate-another-policy">Using the biased policy’s offline logged data to evaluate another policy</a></li>
      <li><a href="#repairing-the-biased-policys-logged-data-with-inverse-probability-weights">Repairing the biased policy’s logged data with inverse probability weights</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Robin van Emden.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: '7d3ef95d72d5a68e705ce87f9919b959',
    indexName: 'nth_iteration_labs_contextual',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
